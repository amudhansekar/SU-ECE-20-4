{"cells":[{"cell_type":"code","source":["%scala\nval configs = Map(\n  \"fs.azure.account.auth.type\" -> \"OAuth\",\n  \"fs.azure.account.oauth.provider.type\" -> \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n  \"fs.azure.account.oauth2.client.id\" -> \"6e252b88-8ce2-4e3b-a45a-234259aa54b0\",\n  \"fs.azure.account.oauth2.client.secret\" -> \"s0:kK7K6DZIQva-Rj@VeXN7bg.1=u.b3\",\n  \"fs.azure.account.oauth2.client.endpoint\" -> \"https://login.microsoftonline.com/6fae808c-42ee-4074-bd88-b51c2011953a/oauth2/token\")\n\n// Optionally, you can add <directory-name> to the source URI of your mount point.\ndbutils.fs.mount(\n  source = \"abfss://recognition@recognitionsa.dfs.core.windows.net/\",\n  mountPoint = \"/mnt/quick_set\",\n  extraConfigs = configs)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">configs: scala.collection.immutable.Map[String,String] = Map(fs.azure.account.oauth2.client.secret -&gt; s0:kK7K6DZIQva-Rj@VeXN7bg.1=u.b3, fs.azure.account.auth.type -&gt; OAuth, fs.azure.account.oauth2.client.endpoint -&gt; https://login.microsoftonline.com/6fae808c-42ee-4074-bd88-b51c2011953a/oauth2/token, fs.azure.account.oauth.provider.type -&gt; org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider, fs.azure.account.oauth2.client.id -&gt; 6e252b88-8ce2-4e3b-a45a-234259aa54b0)\nres0: Boolean = true\n</div>"]}}],"execution_count":1},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["\"\"\"\nAuthors: Ross Pitman, Jack Gularte, Devin DeWitt\nFile: recognition.py\nDate Last Modified: April 24th, 2019\nFor: Panthera Organization\nPurpose: Use computer vision to help determine the number of individual cats in\n            a database.\nOutput: 'score_matrix.csv'; A matrix of similarity scores between each two images\n            within the database\nTo Run: python recogntion.py -work_directory \"path_to_work_directory\"\n\"\"\"\n\n# EXAMPLE USAGES\n################################################################################\n# (if work directory is the directory the file resides in):\n#       python recognition_new.py\n\n# (if work directory is somewhere else you need to specify where):\n#       python recognition_new.py -work_directory \"c:/Users/Jack Gualrte/Desktop\"\n########################### END USAGES #########################################\n\n# NOTES\n################################################################################\n\"\"\"\n    * number of processes has to be less than the number of images in directory\n\n    * if python2 'pip install opencv-contrib-python==3.3.1.11'\n        'i dont think this script will work on python 2 anymore, some packages'\n        ' are python 3 and up'\n    * if python3 then 'pip install opencv-contrib-python==3.4.2.16'\n\"\"\"\n################################ END NOTES #####################################\n\n# IMPORTS\n################################################################################\n# basic python\nimport os, sys\nimport time, datetime\nfrom copy import deepcopy\nimport threading\nimport argparse\nimport glob\nimport re\nimport traceback\nfrom pathlib import Path, PurePath\nimport json\n\n# advanced\nimport cv2\nimport numpy as np\n########################### END IMPORTS ########################################\n\n# CLASS DEFINITION\n################################################################################\nclass Recognition:\n    'This class holds an image-template pair.'\n    'Keeps pairs secure and together thorughout whole process and'\n    'cuts down on code bloat. Also holds the image title split into its'\n    'base characterisitics and the proper cat ID'\n    def __init__(self):\n        self.image_title = \"\"\n        self.image = \"\"\n        self.template_title = \"\"\n        self.template = \"\"\n        self.station = \"\"\n        self.camera = \"\"\n        self.date = \"\"\n        self.time = \"\"\n        self.cat_ID = \"\"\n\n    def add_image(self, image_title, image):\n        self.image_title = image_title\n        self.image = image\n\n    def add_template(self, template_title, template):\n        self.template_title = template_title\n        self.template = template\n\n    def add_title_chars(self, station, camera, date, time):\n        self.station = station\n        self.camera = camera\n        self.date = date\n        self.time = time\n\n    def add_cat_ID(self, cat):\n        self.cat_ID = cat\n########################### END CLASS DEFINITION ###############################\n\n# FUNCTION DEFINITIONS (In Reverse Order of Call)\n################################################################################\ndef check_matrix(rec_list, score_matrix):\n\n\n    hit = 0\n    hit_count = 0\n    miss = 0\n    miss_count = 0\n\n    # traverse rows\n    for row in range(score_matrix.shape[0]):\n\n        primary_cat = rec_list[row].cat_ID\n        primary_title = rec_list[row].image_title\n        #print(\"Cat_ID: {0}; Image: {1}\".format(primary_cat, primary_title))\n\n        # traverse columns\n        for column in range(score_matrix.shape[1]):\n            # don't check the same image.\n            if (row != column):\n                secondary_cat = rec_list[column].cat_ID\n\n                # Pull the 'hit' out of the score matrix\n                if (primary_cat == secondary_cat):\n                    hit = hit + score_matrix[row][column]\n                    hit_count = hit_count + 1\n                else:\n                    miss = miss + score_matrix[row][column]\n                    miss_count = miss_count + 1\n\n    try:\n        print(\"Hits: {0}; Avg. Hit: {1}\".format(hit_count, hit/hit_count))\n    except ZeroDivisionError:\n        print(\"Hits: 0; Avg. Miss: 0\")\n\n    try:\n        print(\"Misses: {0}; Avg. Miss: {1}\".format(miss_count, miss/miss_count))\n    except ZeroDivisionError:\n        print(\"Misses: 0; Avg. Miss: 0\")\n\n################################################################################\ndef normalize_matrix(score_matrix):\n    'Used to normalize the score matrix with respect to the highest value present'\n\n    # get max score\n    max_matrix = score_matrix.max()\n\n    # normalize\n    score_matrix = score_matrix\n\n    # add identity matrix\n    score_matrix = score_matrix + np.identity(len(score_matrix[1]))\n    return score_matrix\n\n################################################################################\ndef write_matches(kp_1, kp_2, good_points, primary_image, secondary_image, image_destination):\n    'This function takes the output of the KNN matches and draws all the matching points'\n    'between the two images. Writes the final product to the output directory'\n\n    # parameters to pass into drawing function\n    draw_params = dict(matchColor = (0,255,0),\n                       singlePointColor = (255,0,0),\n                       flags = 0)\n\n    # draw the matches between two upper pictures and horizontally concatenate\n    result = cv2.drawMatches(\n        primary_image.image,\n        kp_1,\n        secondary_image.image,\n        kp_2,\n        good_points,\n        None,\n        **draw_params) # draw connections\n\n    # use the cv2.drawMatches function to horizontally concatenate and draw no\n    # matching lines. this creates the clean bottom images.\n    result_clean = cv2.drawMatches(\n        primary_image.image,\n        None,\n        secondary_image.image,\n        None,\n        None,\n        None) # don't draw connections\n\n    # This code is Ross Pitman. I dont exactly know what all the constants are but they\n    # create the border and do more image preprocessing\n    row, col= result.shape[:2]\n    bottom = result[row-2:row, 0:col]\n    bordersize = 5\n    result_border = cv2.copyMakeBorder(\n        result,\n        top = bordersize,\n        bottom = bordersize,\n        left = bordersize,\n        right = bordersize,\n        borderType = cv2.BORDER_CONSTANT, value = [0,0,0] )\n\n    # same as above\n    row, col= result_clean.shape[:2]\n    bottom = result_clean[row-2:row, 0:col]\n    bordersize = 5\n    result_clean_border = cv2.copyMakeBorder(\n        result_clean,\n        top = bordersize,\n        bottom = bordersize,\n        left = bordersize,\n        right = bordersize,\n        borderType = cv2.BORDER_CONSTANT, value = [0,0,0] )\n\n    # vertically concatenate the matchesDrawn and clean images created before.\n    result_vertical_concat = np.concatenate(\n        (result_border, result_clean_border),\n        axis = 0)\n\n    # Take the image_destination and turn it into a Path object.\n    # Then add the image names to the new path.\n    # # TODO: For some reason it says the 'image_destination' object is\n    #           a str type at this point in the program even though it is not.\n    #           Look into why.\n    image_path = image_destination.joinpath(str(len(good_points)) +\n    \"___\" +\n    re.sub(\".jpg\", \"\", os.path.basename(primary_image.image_title)) +\n    \"___\" +\n    re.sub(\".jpg\", \".JPG\", os.path.basename(secondary_image.image_title))\n    )\n\n    # Finally, write the finished image to the output folder.\n    cv2.imwrite(str(image_path), result_vertical_concat, [int(cv2.IMWRITE_JPEG_QUALITY), 80])\n################################################################################\ndef score_boosting(primary_image, secondary_image, good_points):\n    'uses image characteristics to boost scores'\n    score = len(good_points)\n\n    if (primary_image.station == secondary_image.station):\n        if (primary_image.camera == secondary_image.camera):\n            if (primary_image.date == secondary_image.date):\n                score = score * float(2.5)\n            else:\n                score = score * float(2.0)\n        else:\n            score = score * float(1.5)\n\n    return score\n################################################################################\ndef match(primary_images, secondary_images, image_destination,\n            start_i, score_matrix, write_threshold, parameters):\n    'main function used for determining matches between two images.'\n    'Finds the sift keypoints/descriptors and uses a KNN based matcher'\n    'to filter out bad keypoints. Writes final output to score_matrix'\n\n    # Begin loop on the primary images to match. Due to multithreading of the\n    # program, this may not be the full set of images.\n    for primary_count in range(len(primary_images)):\n\n        print(\"\\t\\tMatching: \" + os.path.basename(primary_images[primary_count].image_title) + \"\\n\")\n\n        # create mask from template and place over image to reduce ROI\n        mask_1 = cv2.imread(primary_images[primary_count].template_title, -1) \n        mySift = cv2.xfeatures2d.SIFT_create()\n        kp_1, desc_1 = mySift.detectAndCompute(primary_images[primary_count].image, mask_1)\n\n        # parameter setup and create nearest nieghbor matcher\n        index_params = dict(algorithm = 0, trees = 5)\n        search_params = dict()\n        flann = cv2.FlannBasedMatcher(index_params, search_params)\n\n        # Begin nested loop for the images to be matched to. This secondary loop\n        # will always iterate over the full dataset of images.\n        for secondary_count in range(len(secondary_images)):\n\n            # check if same image; if not, go into sophisticated matching\n            if primary_images[primary_count].image_title != secondary_images[secondary_count].image_title:\n\n                 # create mask from template\n                 mask_2 = cv2.imread(secondary_images[secondary_count].template_title, -1)\n                 #cv2.imshow(\"image\",rec_list[secondary_count].image)\n                 time.sleep(10)\n                 kp_2, desc_2 = mySift.detectAndCompute(secondary_images[secondary_count].image, mask_2)\n                \n                 #print(\"Secondary image\", secondary_image)\n                 #cv2.imshow(secondary_image)\n                 \n                #  # This section is for the presentation only, remove later\n                #  temp1 = cv2.resize(rec_list[primary_count].image, (960, 540))\n                #  temp2 = cv2.resize(rec_list[secondary_count].image, (960, 540))\n\n\n                #  horiz = np.hstack((temp1, temp2))\n                #  cv2.imshow(\"matched image to tempalte\", horiz)\n                #  cv2.waitKey(500)\n                #  cv2.destroyAllWindows()\n                 \n                #  # end \n\n                 # check for matches\n                 try:\n                     # Check for similarities between pairs\n                     matches = flann.knnMatch(desc_1, desc_2, k=2)\n\n                     # Use Lowe's ratio test\n                     good_points = []\n                     for m, n in matches:\n                         if m.distance < 0.7 * n.distance:\n                             good_points.append(m)\n\n\n                    #  # RANSAC\n\n                    #  if (int(parameters['config']['ransac'])):\n                    #      src_pts = np.float32([ kp_1[m.queryIdx].pt for m in good_points ]).reshape(-1,1,2)\n                    #      dst_pts = np.float32([ kp_2[m.trainIdx].pt for m in good_points ]).reshape(-1,1,2)\n\n                    #      # used to detect bad keypoints\n                    #      M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n                    #      matchesMask = mask.ravel().tolist()\n\n                    #      h,w = primary_images[primary_count].image.shape[1], primary_images[primary_count].image.shape[0]\n                    #      pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n                    #      dst = cv2.perspectiveTransform(pts,M)\n\n\n                     # take smallest number of keypoints between two images\n                     number_keypoints = 0\n                     if len(kp_1) <= len(kp_2):\n                         number_keypoints = len(kp_1)\n                     else:\n                         number_keypoints = len(kp_2)\n\n                     # score boosting\n                     score = score_boosting(primary_images[primary_count],\n                        secondary_images[secondary_count], good_points)\n\n                     # add the number of good points to score_matrix. start_i is\n                     # passed in as a paramter to ensure that the correct row of the\n                     # score matrix is being written to. Give this index the number\n                     # of 'good_points' from the output of the KNN matcher.\n                     score_matrix[start_i + primary_count][secondary_count] = score\n\n                     # only do image processing if number of good points\n                     # exceeeds threshold\n                     if len(good_points) > write_threshold:\n                         write_matches(kp_1, kp_2, good_points,\n                            primary_images[primary_count], secondary_images[secondary_count],\n                            image_destination)\n\n                 except cv2.error as e:\n                     print('\\n\\t\\tERROR: {0}\\n'.format(e))\n                     print(\"\\t\\tError matching: \" + os.path.basename(primary_images[primary_count].image_title) +\n                         \" and \" + os.path.basename(secondary_images[secondary_count].image_title) + \"\\n\")\n\n    return score_matrix\n\n################################################################################\ndef slice_generator(\n        sequence_length,\n        n_blocks):\n    \"\"\" Creates a generator to get start/end indexes for dividing a\n        sequence_length into n blocks\n    \"\"\"\n    return ((int(round((b - 1) * sequence_length/n_blocks)),\n             int(round(b * sequence_length/n_blocks)))\n            for b in range(1, n_blocks+1))\n\n################################################################################\ndef match_multi(primary_images, image_destination, n_threads, write_threshold, parameters):\n    'Wrapper function for the \"match\". This also controls the multithreading'\n    'if the user has declared to use multiple threads'\n\n    # deep copy the primary_images for secondary images\n    secondary_images = deepcopy(primary_images)\n\n    # init score_matrix\n    num_pictures = len(primary_images)\n    score_matrix = np.zeros(shape = (num_pictures, num_pictures))\n\n    # prep for multiprocessing; slices is a 2D array that specifies the\n    # start and end array index for each program thread about to be created\n    slices = slice_generator(num_pictures, n_threads)\n    thread_list = list()\n\n    print(\"\\tImages to pattern match: {0}\\n\".format(str(num_pictures)))\n\n    # start threading\n    for i, (start_i, end_i) in enumerate(slices):\n\n        thread = threading.Thread(target = match,\n                    args = (primary_images[start_i: end_i],\n                            secondary_images,\n                            image_destination,\n                            start_i,\n                            score_matrix,\n                            write_threshold,\n                            parameters))\n        thread.start()\n        thread_list.append(thread)\n\n    for thread in thread_list:\n        thread.join()\n\n    return score_matrix\n################################################################################\ndef add_cat_ID(rec_list, cluster_path):\n\n    # create the list\n    import pandas as pd\n    csv_file = pd.read_csv(cluster_path)\n    image_names = list(csv_file['Image Name'])\n    cat_ID_list = list(csv_file['Cat ID'])\n\n    for count in range(len(rec_list)):\n        image = os.path.basename(rec_list[count].image_title)\n        try:\n            image_index = image_names.index(image)\n        except ValueError:\n            print('\\tSomething is wrong with cluster_table file. Image name is not present.')\n\n        cat_ID = cat_ID_list[image_index]\n        rec_list[count].add_cat_ID(cat_ID)\n\n    return rec_list\n\n################################################################################\ndef crop(event, x, y, flags, param):\n\n    global ref_points, cropping\n\n    if event == cv2.EVENT_LBUTTONDOWN:\n        ref_points = [(x, y)]\n        cropping = True\n\n    elif event == cv2.EVENT_LBUTTONUP:\n\n        ref_points.append((x, y))\n        cropping = False\n\n        cv2.rectangle(param, ref_points[0], ref_points[1], (0, 255, 0), 2)\n        cv2.imshow(\"image\", param)\n################################################################################\ndef variance_of_laplacian(image):\n\t# compute the Laplacian of the image and then return the focus\n\t# measure, which is simply the variance of the Laplacian\n\treturn cv2.Laplacian(image, cv2.CV_64F).var()\n################################################################################\ndef manual_roi(rec_list, image_source):\n\n    cropping = False\n    count = 0\n\n    #todo: do a check and make sure temp_templates folder isn't already created\n    temp_templates = image_source.parents[1] / \"temp_templates/\"\n    \n    if (not os.path.exists(temp_templates)):\n        os.mkdir(temp_templates)\n\n    for i in glob.iglob(str(image_source)):\n        print(i)\n        # read in image and change size so it doesn't expand to whole screen; make a copy\n        image = cv2.imread(i)\n        image = cv2.resize(image, (960, 540))\n        rec_list[count].add_image(i, image)\n        image_clone = image.copy()\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        fm = variance_of_laplacian(gray)\n        text = \"Not Blurry\"\n\n        #if(fm <= 100)\n         #   text = \"blurry\"\n        cv2.putText(image, \"{}: {:.2f}\".format(text,fm), (10,30),\n        cv2.FONT_HERSHEY_SIMPLEX, .8, (0,0,255),3)\n        cv2.imshow(\"Image\", image)\n        key = cv2.waitKey(0)\n\n#gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        #fm = variance_of_laplacian(gray)\n\t    #text = \"Not Blurry\"\n\n        #if(fm < 100)\n           #text = \"Blurry\"\n         #cv2.putText(image, \"{}: {:.2f}\".format(text, fm), (10, 30),\n\t\t #cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 3)\n\t     #cv2.imshow(\"Image\", image)\n\t     #key = cv2.waitKey(0)\n                \n\n        # create an empy numpy array for the mask. Will be all black to start\n        mask = np.zeros(image.shape, dtype = np.bool)\n\n        # set up event callback for mouse click\n        cv2.namedWindow(\"image\")\n        cv2.setMouseCallback(\"image\", crop, image)\n\n        while True:\n            cv2.imshow(\"image\", image)\n            key = cv2.waitKey(1) & 0xFF\n\n            if key == ord(\"r\"):\n                image = image_clone.copy()\n\n            if key == ord(\"c\"):\n                break\n\n        if len(ref_points) == 2:\n\n            # Using the coordinates from the moust click. Thurn an area of the mask to white.\n            mask[ref_points[0][1]:ref_points[1][1], ref_points[0][0]:ref_points[1][0]] = True\n            image = image_clone * (mask.astype(image_clone.dtype))\n            #cv2.imshow(\"new image\",image)\n            locations = np.where(image != 0)\n            image[locations[0], locations[1]] = (255, 255, 255)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            \n            \n            \n            template_name = Path(i).with_suffix('.BMP')\n            print(template_name.name)\n            template_path = temp_templates / template_name.name\n            print(template_path)\n            cv2.imwrite(str(template_path), image, [int(cv2.IMWRITE_JPEG_QUALITY), 80])\n            rec_list[count].add_template(str(template_path), image)\n        #cv2.imshow(\"hope\",image)\n        #cv2.imshow(\"hopetoo\",rec_list[count].image)\n        #time.sleep(2)\n        cv2.destroyAllWindows()\n        count = count + 1\n\n    return rec_list\n\n################################################################################\ndef add_templates(rec_list, template_source):\n    'Used for adding the premade templates to the recognition class if'\n    'the user has them.'\n    count = 0\n\n    # add in template\n    for t in glob.iglob(str(template_source)):\n\n        template = cv2.imread(t)\n        rec_list[count].add_template(t, template)\n\n        # This section is for the presentation only, remove later\n        temp1 = cv2.resize(rec_list[count].image, (960, 540))\n        temp2 = cv2.resize(template, (960, 540))\n\n\n        horiz = np.hstack((temp1, temp2))\n        cv2.imshow(\"matched image to template\", horiz)\n        cv2.waitKey(0)\n        cv2.destroyAllWindows()\n        # end \n\n        count = count + 1\n\n    return rec_list\n################################################################################\ndef getTitleChars(title):\n    'Used to pull the characteristics out of the image title name'\n    title_chars = title.split(\"__\")\n    station = title_chars[1]\n    camera = title_chars[2]\n    date = title_chars[3]\n    # don't want the last 7 characters\n    time = title_chars[4][:-7]\n\n    return station, camera, date, time\n################################################################################\ndef init_Recognition(image_source, template_source):\n    'Used to initalize a recongition object for each template/image pair'\n    # # TODO: create a function that verifies the image and template names match\n\n    print(\"image_source: \")\n    print(image_source)\n\n    rec_list = []\n    count = 0\n\n    # print(\"\\n\\nReading \"\"C:\\\\Users\\\\Phil\\\\SU-ECE-19-7-master\\\\Image_Sets\\\\quick_set\\\\images\\\\01__Station4__Camera2__2012-07-22__00-53-20(1).JPG\"\" with cv2.imread\")\n    # #store all images in the quick_set image folder\n    # images = cv2.imread(glob.glob(\"C:/User/Phil/SU-ECE-19-7-master/Image_Sets/quick_set/images/01__Station05__Camera1__2012-6-14__5-38-10(2).JPG\"))\n    # print(\"...........done\")\n\n    # add images and templates in a parallel for-loop \n    for i in glob.iglob(str(image_source)):\n        print(\"i: \")\n        print(i)\n        # add new Recognition object to list\n        rec_list.append(Recognition())\n\n        # add image title and image to object\n        image = cv2.imread(i)\n        cv2.imshow(image)\n\n        rec_list[count].add_image(i, image)\n\n        # get title characteristics\n        station, camera, date, time = getTitleChars(i)\n        rec_list[count].add_title_chars(station, camera, date, time)\n\n        # increment count\n        count = count + 1\n\n    # return the list of recognition objects\n    return rec_list\n\n########################### END FUNCTION DEFINITIONS ###########################"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["# MAIN\n################################################################################\nif __name__ == \"__main__\":\n\n    # # set up via command line\n    # parser = argparse.ArgumentParser()\n\n    # # easy config\n    # parser.add_argument(\"-work_directory\", type = Path, required = False, default = None)\n\n    # # manual config\n    # parser.add_argument(\"-image_source\", type = Path, required = True, default = None)\n    # parser.add_argument(\"-template_source\", type = Path, required = False, default = None)\n    # parser.add_argument(\"-config_source\", type = Path, required = False, default = None)\n    # parser.add_argument(\"-cluster_source\", type = Path, required = False, default = None)\n    # parser.add_argument(\"-destination\", type = Path, required = False, default = Path.cwd())\n    # parser.add_argument(\"-num_threads\", type = int, required = False, default = 1)\n    # parser.add_argument(\"-write_threshold\", type = int, required = False, default = 60)\n\n    # args = vars(parser.parse_args())\n\n    # print(\"\\n\\nARGS\\n\")\n    # for k,v in args.items():\n    #     print(k,v)\n    # print(\"\\n\\n\")\n\n    # initialize depending on input arguments\n    paths = {'images': '', 'templates': '', 'config': '', 'cluster': '', 'destination': ''}\n\n    paths['images'] = \"/mnt/quick_set/images/*\"\n    paths['templates'] = \"/mnt/quick_set/templates/*\"\n    paths['config'] = \"/mnt/quick_set/config.json\"\n    paths['cluster'] = None\n    paths['destination'] = \"/mnt/destination\"\n    n_threads = 1\n    write_threshold = 30\n\n    print(\"\\n\\nPATHS\\n\")\n    for k,v in paths.items():\n        print(k,v)\n    print(\"\\n\\n\")\n\n    # TODO: change this to fit new command line argument scheme\n    # Use the config.json file to import variable parameters\n #   with open(\"C:/Users/Phil/SU-ECE-19-7-master/Image_Sets/quick_set/config.json\") as config_file:\n #      parameters = json.load(config_file)\n\n    parameters = {\n        \"config\" : {\n            \"templating\": \"1\",\n            \"ransac\": \"0\"\n        },\n        \"score_boosting\" : {\n            \"station_score\": \"1.5\",\n            \"camera_score\": \"2.0\",\n            \"date_score\": \"2.5\",\n            \"time_score\": \"3.0\",\n            \"time_delta\": \"20\"\n        }\n    }\n\n    print(\"\\n\\nPARAMETERS\\n\")\n    for k,v in parameters.items():\n        print(k,v)\n    print(\"\\n\\n\")\n\n    # initialize the array of Recognition objects for the images\n    rec_list = init_Recognition(paths['images'], paths['templates'])\n\n    # if (int(parameters['config']['templating']) == 1):\n    print('\\n\\tUsing premade templates...\\n')\n    rec_list = add_templates(rec_list, paths['templates'])\n    # else:\n    #     print('\\n\\tUsing the manual templating function...\\n')\n    #     rec_list = manual_roi(rec_list, paths['images'])\n\n    # Get cat_ID information from cluster table\n    if (paths['cluster'] != None):\n        print(\"\\n\\tLoading information from cluster table...\\n\")\n        # add in the cat ID data if available\n        rec_list = add_cat_ID(rec_list, paths['cluster'])\n    \n    \n    # START\n    print(\"\\tstarting matching process...\\n\")\n    start = time.time()\n    score_matrix = match_multi(rec_list, paths['destination'], n_threads, write_threshold, parameters)\n    end = time.time()\n    print(\"\\tTime took to run: \" + str((end - start)))\n\n    # Normalize scores in matrix\n    score_matrix = normalize_matrix(score_matrix)\n\n    # write the score matrix to a .csv file\n    print(\"\\n\\tWriting score_matrix to 'score_matrix.csv' to the destination folder...\\n\")\n    np.savetxt(paths['destination'].joinpath('score_matrix.csv'), score_matrix, delimiter = \",\")\n\n\n    # check matrix for average hit/miss scores\n    if (paths['cluster'] != None):\n        print(\"Checking matrix...\")\n        check_matrix(rec_list, score_matrix)\n\n    print('\\n\\tDone.\\n')\n    \n############################### END  MAIN ######################################\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-785537239781955&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     90</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     91</span>     <span class=\"ansired\"># Normalize scores in matrix</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 92</span><span class=\"ansiyellow\">     </span>score_matrix <span class=\"ansiyellow\">=</span> normalize_matrix<span class=\"ansiyellow\">(</span>score_matrix<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     93</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     94</span>     <span class=\"ansired\"># write the score matrix to a .csv file</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">&lt;command-785537239781954&gt;</span> in <span class=\"ansicyan\">normalize_matrix</span><span class=\"ansiblue\">(score_matrix)</span>\n<span class=\"ansigreen\">    133</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    134</span>     <span class=\"ansired\"># get max score</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 135</span><span class=\"ansiyellow\">     </span>max_matrix <span class=\"ansiyellow\">=</span> score_matrix<span class=\"ansiyellow\">.</span>max<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    136</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    137</span>     <span class=\"ansired\"># normalize</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/lib/python3.5/site-packages/numpy/core/_methods.py</span> in <span class=\"ansicyan\">_amax</span><span class=\"ansiblue\">(a, axis, out, keepdims)</span>\n<span class=\"ansigreen\">     24</span> <span class=\"ansired\"># small reductions</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     25</span> <span class=\"ansigreen\">def</span> _amax<span class=\"ansiyellow\">(</span>a<span class=\"ansiyellow\">,</span> axis<span class=\"ansiyellow\">=</span><span class=\"ansigreen\">None</span><span class=\"ansiyellow\">,</span> out<span class=\"ansiyellow\">=</span><span class=\"ansigreen\">None</span><span class=\"ansiyellow\">,</span> keepdims<span class=\"ansiyellow\">=</span><span class=\"ansigreen\">False</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 26</span><span class=\"ansiyellow\">     </span><span class=\"ansigreen\">return</span> umr_maximum<span class=\"ansiyellow\">(</span>a<span class=\"ansiyellow\">,</span> axis<span class=\"ansiyellow\">,</span> <span class=\"ansigreen\">None</span><span class=\"ansiyellow\">,</span> out<span class=\"ansiyellow\">,</span> keepdims<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     27</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     28</span> <span class=\"ansigreen\">def</span> _amin<span class=\"ansiyellow\">(</span>a<span class=\"ansiyellow\">,</span> axis<span class=\"ansiyellow\">=</span><span class=\"ansigreen\">None</span><span class=\"ansiyellow\">,</span> out<span class=\"ansiyellow\">=</span><span class=\"ansigreen\">None</span><span class=\"ansiyellow\">,</span> keepdims<span class=\"ansiyellow\">=</span><span class=\"ansigreen\">False</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">ValueError</span>: zero-size array to reduction operation maximum which has no identity</div>"]}}],"execution_count":4},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":5}],"metadata":{"name":"april09-recognition-attempt","notebookId":785537239781935},"nbformat":4,"nbformat_minor":0}
